


from dotenv import load_dotenv
import os

def carregar_chaves_api():
    load_dotenv()

    groq_api_key = os.getenv('GROQ_API_KEY')
    lang_smith_tracing = os.getenv('LANGSMITH_TRACING')
    lang_smith_api_key = os.getenv('LANGSMITH_TRACING')


def imprimir_chaves_api():
    print(f"GROQ_API_KEY: {groq_api_key}")
    print(f'LANGSMITH_TRACING: {lang_smith_tracing}')
    print(f'LANGSMITH_API_KEY: {lang_smith_api_key}')


carregar_chaves_api()





from langchain_groq import ChatGroq

class Modelos:
    def __init__(self):
        self.models = {
            'google_gemma': 'gemma2-9b-it',
            'meta_llama': 'llama-3.3-70b-versatile',
            'deep_seek': 'deepseek-r1-distill-llama-70b-specdec'
        }

        self.google_llm = ChatGroq(model=self.models['google_gemma'])
        self.meta_llm = ChatGroq(model=self.models['meta_llama'])
        self.deep_seek_llm = ChatGroq(model=self.models['deep_seek'])

    def imprimir_llms_carregadas(self):
        print(f'GOOGLE_LLM: {self.google_llm}\n')
        print(f'META_LLM: {self.meta_llm}\n')
        print(f'DEEP_SEEK_LLM: {self.deep_seek_llm}\n')

new_obj = Modelos()
new_obj.imprimir_llms_carregadas()





from langchain_community.document_loaders import PyPDFLoader
import os

# Lista para armazenar os documentos carregados
diretorio_base='../data/minuta/'
docs_list = []
i = 1

# Itera sobre os arquivos no diretório e carrega os PDFs
for nome_arquivo in os.listdir(diretorio_base):
    if nome_arquivo.endswith(".pdf"):  # Verifica se o arquivo é um PDF
        caminho_pdf = os.path.join(diretorio_base, nome_arquivo)

        loader = PyPDFLoader(caminho_pdf)

        print(f'loader: {loader}\n\n')
                
        docs = await loader.aload()

        print(f'docs_loader: {docs[0].page_content[:100]}\n\n')
        print(docs[0].metadata)
        
        docs_list.append(docs)

        print(i)
        # i += 1
    if i == 1:
        break






from langchain_text_splitters import RecursiveCharacterTextSplitter

# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100000,  # chunk size (characters)
    chunk_overlap=20000,  # chunk overlap (characters)
    add_start_index=True,  # track index in original document
)
all_splits = text_splitter.split_documents(docs_list[0])

print(len(all_splits))  # ler tamanho total de divisões
len(all_splits[0].page_content)  # ler tamanho total de divisões da pagina 0

all_splits[10].metadata   # metadados do doc





from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

print(embeddings)





from langchain_community.vectorstores import FAISS

vector_store = FAISS.from_documents(all_splits, embeddings)

print(vector_store[:1])





from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)

# print(all_splits[0])
document_ids = vector_store.add_documents(documents=all_splits[:100])

print(document_ids)





from langchain import hub

# Define prompt for question-answering
prompt = hub.pull("rlm/rag-prompt")





from langgraph.graph import START, StateGraph
from typing_extensions import List, TypedDict
from langchain_core.documents import Document

# Define state for application
class State(TypedDict):
    question: str
    context: List[Document]
    answer: str


# Define application steps
def retrieve(state: State):
    retrieved_docs = vector_store.similarity_search(state["question"])
    return {"context": retrieved_docs}


def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = new_obj.google_llm.invoke(messages)
    return {"answer": response.content}


# Compile application and test
graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()


response = graph.invoke({"question": "que tipo de documento é este?"})
print(response["answer"])


response = graph.invoke({"question": "me diga de que empresa pertence"})
print(response["answer"])


response = graph.invoke({"question": "identifique alguma informação sobre a empresa ou algo semelhante"})
print(response["answer"])



